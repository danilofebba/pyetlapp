{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508db180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe7d87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars\", \"/opt/spark/jars/aws-java-sdk-bundle-1.12.505.jar,/opt/spark/jars/hadoop-aws-3.3.4.jar,/opt/spark/jars/hadoop-common-3.3.4.jar\") \\\n",
    "    .config('spark.master', 'spark://192.168.0.11:7077') \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .config(\"spark.cores.max\", 2) \\\n",
    "    .config(\"spark.executor.cores\", 2) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"http://192.168.0.40:9000\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", os.environ['AWS_ACCESS_KEY_ID']) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", os.environ['AWS_SECRET_ACCESS_KEY']) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0605fb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "    create table tb_01 (\n",
    "        id varchar(36),\n",
    "        code integer,\n",
    "        option char(8),\n",
    "        description string,\n",
    "        value double,\n",
    "        rate double,\n",
    "        created_at timestamp,\n",
    "        updated_at timestamp,\n",
    "        status boolean\n",
    "    ) partitioned by (date date) stored as parquet location 's3a://ddfs-bucket-01/tb_01';\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a929b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "spark.sql(\"\"\"\n",
    "    --show databases;\n",
    "    show tables;\n",
    "\"\"\").show(truncate=False)\n",
    "'''\n",
    "\n",
    "'''\n",
    "spark.sql(\"\"\"\n",
    "    --describe database default;\n",
    "    describe table default.tb_01\n",
    "\"\"\").show(truncate=False)\n",
    "'''\n",
    "\n",
    "# add all partition\n",
    "spark.sql(\"\"\"\n",
    "    repair table tb_01;\n",
    "\"\"\")\n",
    "\n",
    "'''\n",
    "# add partition\n",
    "spark.sql(\"\"\"\n",
    "    alter table tb_01 add if not exists partition(date = '2022-05-21') partition(date = '2022-11-14');\n",
    "\"\"\")\n",
    "'''\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    select count(*) from tb_01\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "'''\n",
    "spark.sql(\"\"\"\n",
    "    show partitions tb_01;\n",
    "\"\"\").show(truncate=False)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce69b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
